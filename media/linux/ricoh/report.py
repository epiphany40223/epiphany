#!/usr/bin/env python3

# Script which runs daily to update staff print totals
# based on csvs generated by the printer itself.

import re
import os
import pytz
import sqlite3
import argparse
import datetime

import openpyxl
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

# Find the path to the ECC module (by finding the root of the git
# tree).  This is robust, but it's a little clunky. :-\
try:
    import sys
    import subprocess
    out = subprocess.run(['git', 'rev-parse', '--show-toplevel'],
                         capture_output=True)
    dirname = out.stdout.decode('utf-8').strip()
    if not dirname:
        raise Exception("Could not find git root.  Are you outside the git tree?")

    moddir  = os.path.join(dirname, 'python')
    sys.path.insert(0, moddir)
    import ECC
except Exception as e:
    sys.stderr.write("=== ERROR: Could not find common ECC Python module directory\n")
    sys.stderr.write(f"{e}\n")
    exit(1)

###########################################################

def setup_cli_args():
    parser = argparse.ArgumentParser(description='Import Ricoh data into an SQLite3 database.')
    parser.add_argument('--xlsx',
                        default='printlog.xlsx',
                        help='XLSX file to write')
    parser.add_argument('--db',
                        required=True,
                        help='SQLite3 database from which to read values')
    parser.add_argument('--first',
                        help='First local timestamp "yyyy-mm-dd[ hh:mm:ss]"')
    parser.add_argument('--last',
                        help='Last local timestamp "yyyy-mm-dd[ hh:mm:ss]"')

    parser.add_argument('--logfile',
                        default=None,
                        help='Optional output logfile')

    parser.add_argument('--slack-token-filename',
                        required=False, #TODO: switch back to True after debugging
                        help='File containing the Slack bot authorization token')

    parser.add_argument('--verbose',
                        default=False,
                        action='store_true',
                        help='Enable verbose output')
    parser.add_argument('--debug',
                        default=False,
                        action='store_true',
                        help='Enable extra debugging')

    args = parser.parse_args()

    # Sanity check
    if not os.path.exists(args.db):
        print(f"ERROR: SQLite3 database does not exist {args.db}")
        exit(1)

    # Convert the string timestamp to a Python datetime in GMT
    def _convert_timestamp(timestamp):
        # The timestamp must be of one of the following forms:
        # yyyy-mm-dd
        # yyyy-mm-dd hh:mm:ss
        if re.match('\d\d\d\d-\d\d-\d\d$', timestamp):
            ts = datetime.datetime.strptime(timestamp,
                                            "%Y-%m-%d")

        elif re.match('\d\d\d\d-\d\d-\d\d \d\d:\d\d:\d\d$', timestamp):
            ts = datetime.datetime.strptime(timestamp,
                                            "%Y-%m-%d %H:%M:%S")

        else:
            print(f"ERROR: Timestamp is not in expected format: {timestamp}")
            exit(1)

        # Convert to GMT
        return ts.astimezone(gmt)

    # Normalize the two timestamps
    gmt = pytz.timezone("GMT")
    if args.first:
        args.first = _convert_timestamp(args.first)
    if args.last:
        args.last  = _convert_timestamp(args.last)

    return args

###########################################################

# Find the first and last timestamps in the database
def find_first_last_timestamp(log, conn):
    def _doit(direction):
        c    = conn.cursor()
        sql  = f'SELECT timestamp FROM printlog GROUP BY timestamp ORDER BY datetime(timestamp) {direction} LIMIT 1'
        log.debug(f"SQL: {sql}")
        c.execute(sql)
        rows = c.fetchall()

        return rows

    log.debug(f"Looking for first and last timestamps")
    rows = _doit("ASC")
    if len(rows) < 1:
        log.verbose("Database is empty: there is no first and last timestamp")
        return None, None

    first = datetime.datetime.fromisoformat(rows[0]['timestamp'])
    log.debug(f"Found first timestamp: {first}")

    rows = _doit("DESC")
    last = datetime.datetime.fromisoformat(rows[0]['timestamp'])
    log.debug(f"Found last timestamp: {last}")

    return first, last

# Find the first timestamp in the database that is >= the requested
# timestamp. Remember that the timestamp parameter is in local time, but
# timestamps in the database are in GMT.
def find_timestamp(log, conn, target_timestamp):
    c    = conn.cursor()
    log.debug(f"Looking for timestamp: {target_timestamp}")
    sql  = f'SELECT timestamp FROM printlog WHERE datetime(timestamp)>=datetime(?) GROUP BY timestamp ORDER BY datetime(timestamp) LIMIT 1'
    log.debug(f"SQL: {sql}")
    c.execute(sql, [target_timestamp])
    rows = c.fetchall()

    log.debug(f"Found {len(rows)} rows greater than {target_timestamp}")
    if len(rows) < 1:
        return None
    return datetime.datetime.fromisoformat(rows[0]['timestamp'])

###########################################################

def fetch_depts_at_timestamp(log, conn, timestamp):
    c     = conn.cursor()
    c.execute('SELECT department FROM printlog WHERE timestamp=?', [timestamp])
    rows  = c.fetchall()
    depts = { row['department'] : timestamp for row in rows }

    log.debug(f"Found {len(depts)} departments at timestamp {timestamp}: {depts}")
    return depts

###########################################################

def compute_depts_union(log, depts_first, depts_last):
    def _compute(depts, label):
        for dept, timestamp in depts.items():
            if dept not in depts_union:
                depts_union[dept] = {
                    'id' : dept,
                }
            depts_union[dept][label] = timestamp

    depts_union = dict()
    _compute(depts_first, "first")
    _compute(depts_last, "last")

    log.debug(f"Union of all departments: {depts_union}")
    return depts_union

###########################################################

# Examine each department:
#
# - if we are missing a "first" timestamp, this means that the department was
#   added between the two timestamps specified on the command line.  Go find
#   the first timestamp where we have data for that new department.
# - if we are missing a "last" timestamp, this means that the department was
#   deleted between the two timestamps specified on the command line.  Go find
#   the last timestamp where we have data for that now-deleted department.
def fetch_missing_timestamps(log, conn, timestamp_first, timestamp_last, depts):
    c = conn.cursor()

    for dept in depts.values():
        label = None
        if 'first' not in dept:
            # Find first timestamp with this department
            desc      = ''
            timestamp = timestamp_first
            label     = 'first'

        elif 'last' not in dept:
            # Find last timestamp with this department
            desc      = 'DESC'
            timestamp = timestamp_last
            label     = 'last'

        if label is not None:
            sql         = f'SELECT timestamp FROM printlog WHERE department=? AND datetime(timestamp)>=datetime(?) GROUP BY timestamp ORDER BY datetime(timestamp) {desc} LIMIT 1'
            values      = [ dept['id'], timestamp ]
            c.execute(sql, values)
            rows        = c.fetchall()
            dept[label] = datetime.datetime.fromisoformat(rows[0]['timestamp'])

    log.debug(f"Depts with all timestamps filled in: {depts}")

###########################################################

# Get the data for each department and timestamp.
#
# We could do something clever to minimize the number of SQL lookups, but
# the Python is simpler to read if we just have a simple loop to look
# up each one.  For a database/application this small, the performance
# difference is negligible.
def fetch_data(log, conn, depts):
    def _fetch(log, id, timestamp):
        sql    = 'SELECT * FROM printlog WHERE department=? AND timestamp=?'
        values = [ id, timestamp ]
        c.execute(sql, values)
        rows = c.fetchall()
        return rows[0]

    c = conn.cursor()

    for dept, item in depts.items():
        log.debug(item)
        data = _fetch(log, dept, item['first'])
        item['first_data'] = data

        data = _fetch(log, dept, item['last'])
        item['last_data'] = data

    log.debug(f"Full set of data: {depts}")

###########################################################

# finds the deltas between the sets of data, returns a list of department deltas
def compute_deltas(log, depts):
    for dept, item in depts.items():
        # Assumption: the column names are the same between the old and new data
        delta = dict()
        for col in item['first_data'].keys():
            data_old = item['first_data'][col]
            data_new = item['last_data'][col]
            # do not compare any data that is not an integer
            if type(data_old) == str and type(data_new) == str:
                continue
            delta[col] = data_new - data_old
        item['deltas'] = delta

    log.debug("Computed deltas")
    log.debug(depts)

###########################################################

# Writes the deltas to an XLSX
def write_to_xlsx(log, fields, depts, filename, timestamp_first, timestamp_last):
    wb = Workbook()
    ws = wb.active

    # Reserve some blank rows for the titles
    titles = [
        'Ricoh printer counts by department',
        # "None" renders the timestamp in the local timezone, and
        # ctime() puts it in a pleasing human-readable format.
        f'{timestamp_first.astimezone(None).ctime()} through {timestamp_last.astimezone(None).ctime()}',
    ]
    for i in range(len(titles) + 1):
        ws.append([])

    # Add a row of the column names
    column_names = [ 'Department', 'Name', 'Start', 'End' ]
    first_dept = list(depts.values())[0]
    item = first_dept['deltas']
    for field in fields:
        if field in item and type(item[field]) != str:
            column_names.append(field)

            # The "bwTotal" and "colorTotal" columns are special:
            # we'll add another column after each of those two, which
            # will be a computed value of this department's percentage
            # of the overall total.
            if field == 'bwTotal' or field == 'colorTotal':
                column_names.append(f'% of overall {field}')

    ws.append(column_names)

    # Get the last row number.  The next row is the first row of data.
    for row in ws.rows:
        # row is a tuple of cells
        cell = row[0]
        last_row = cell.row
    first_data_row = last_row + 1
    last_data_row = first_data_row + len(depts.keys()) - 1

    # Now add a row for each set of delta data
    row = first_data_row
    percentage_cols = list()
    for dept_id in sorted(depts.keys()):
        item  = depts[dept_id]
        # "None" renders these Python datetimes in the local timezone
        first = item['first'].astimezone(None)
        last  = item['last'].astimezone(None)

        # Make sure to render the times the way that we want (e.g., ctime()),
        # otherwise OpenPyXL will render them as GMT.
        data  = [ dept_id, item['first_data']['name'],
                  first.ctime(), last.ctime() ]
        for field in fields:
            if field in item['deltas'] and type(item['deltas'][field]) != str:
                data.append(item['deltas'][field])

                # Just like above, the "bwTotal" and "colorTotal"
                # columns are special: we add in a column for this
                # department's percentage of the overall total.  Note:
                # we don't need to compute this value ourselves -- we
                # just put in an Excel formula to calculate it.
                col_letter = chr(ord('A') + len(data) - 1)
                if field == 'bwTotal' or field == 'colorTotal':
                    value = f'={col_letter}{row}/sum({col_letter}{first_data_row}:{col_letter}{last_data_row})'
                    data.append(value)

                    col_letter = chr(ord('A') + len(data) - 1)
                    if col_letter not in percentage_cols:
                        percentage_cols.append(col_letter)

        ws.append(data)
        row += 1

    # automatically adjusts the width of every column to fit the data in it
    dims = dict()
    for row in ws.rows:
        for cell in row:
            if cell.value:
                dims[cell.column_letter] = max((dims.get(cell.column_letter, 0), len(str(cell.value))))+.5
    for col, value in dims.items():
        ws.column_dimensions[col].width = value

    # Get the last column letter
    for cell in ws[1]:
        last_col = cell.column_letter

    # Get the last row number
    for row in ws.rows:
        # row is a tuple of cells
        cell = row[0]
        last_row = cell.row

    # Set the title rows to a nice value
    for row, title in enumerate(titles):
        ws.merge_cells(f'A{row+1}:{last_col}{row+1}')
        ws[f'A{row+1}'] = title

    # Set the first several rows to be a specific font/color
    title_font = Font(color='FAFAF9')
    title_fill = PatternFill(fgColor='228B22', fill_type='solid')
    title_align = Alignment(horizontal='center')

    # Set the two percentage columns to have "Percent" formats
    for col in percentage_cols:
        for row in range(first_data_row, last_row + 1):
            address = f'{col}{row}'
            ws[address].style = 'Percent'

    # We color all the title rows, the blank row after the titles, and
    # the row with all the column headings
    for row in ws.iter_rows(min_row=1, max_row=len(titles) + 2):
        for cell in row:
            cell.fill = title_fill
            cell.font = title_font

    wb.save(filename)
    log.info(f"Wrote {filename}")

###########################################################

def open_db(log, filename):
    conn = sqlite3.connect(filename)
    conn.row_factory = sqlite3.Row

    # Fetch a list of the SQL table field names.  We do this because we know
    # the SQL table was created with fields in a specific order (per
    # db_insert.py), and we want to output the XLSX with the fields in that
    # order.  Specifically: we use dictionaries (and dictionary-like SQL rows)
    # to store field data here in this script, and those will be iterated in
    # effectively random order.  So we obtain these field names in a list
    # as a stable ordering for emitting the final XLSX.
    c      = conn.cursor()
    c.execute("SELECT * FROM printlog ORDER BY key LIMIT 1")
    fields = [ description[0] for description in c.description ]
    log.debug(f"SQL field names: {fields}")

    return conn, fields

###########################################################

def main():
    args = setup_cli_args()
    log  = ECC.setup_logging(info=args.verbose,
                             debug=args.debug,
                             logfile=args.logfile, rotate=True,
                             slack_token_filename=args.slack_token_filename)

    conn, fields = open_db(log, args.db)

    # If the first and last timestamps were not specified, use the first and
    # last timestamps in the database.
    first, last = find_first_last_timestamp(log, conn)
    if args.first == None:
        args.first = first
    if args.last == None:
        args.last = last

    # Find the earliest timestamp in the database that is greater than or equal
    # to the timestamps that were specified on the command line.
    def _check(label, timestamp):
        ts = find_timestamp(log, conn, timestamp)
        if ts is None:
            log.error(f"Could not find suitable timestamp in database for {timestamp}")
            exit(1)
        return ts

    timestamp_first = _check('first', args.first)
    timestamp_last  = _check('last', args.last)

    # Fetch all the departments that are available at those two timestamps.
    depts_first = fetch_depts_at_timestamp(log, conn, timestamp_first)
    depts_last  = fetch_depts_at_timestamp(log, conn, timestamp_last)

    # We may have found a different set of departments at the first and last
    # timestamps (e.g., if a department was added or deleted between the two
    # timestamps).  Compute the union of departments that we found.
    depts = compute_depts_union(log, depts_first, depts_last)

    # Go fetch any missing data (i.e., a department for which we have only a
    # first or last set of data).
    fetch_missing_timestamps(log, conn, timestamp_first, timestamp_last, depts)

    # Now that we have all the timestamps, fetch all the data
    fetch_data(log, conn, depts)

    # Now that we have first and last data for every single department, compute
    # the deltas for all of them.
    compute_deltas(log, depts)

    write_to_xlsx(log, fields, depts, args.xlsx, timestamp_first, timestamp_last)

    conn.close()

main()
