#!/usr/bin/env python3

# Script which runs daily to update staff print totals
# based on csvs generated by the printer itself.

import re
import os
import sys
import pytz
import tzlocal
import sqlite3
import argparse
import datetime

# We assume that there is a "ecc-python-modules" sym link in this
# directory that points to the directory with ECC.py and friends.
moddir = os.path.join(os.getcwd(), 'ecc-python-modules')
if not os.path.exists(moddir):
    print("ERROR: Could not find the ecc-python-modules directory.")
    print("ERROR: Please make a ecc-python-modules sym link and run again.")
    exit(1)
# On MS Windows, git checks out sym links as a file with a single-line
# string containing the name of the file that the sym link points to.
if os.path.isfile(moddir):
    with open(moddir) as fp:
        dir = fp.readlines()
    moddir = os.path.join(os.getcwd(), dir[0])

sys.path.insert(0, moddir)

import ECC
import Google

from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

import openpyxl
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter

###########################################################

def setup_cli_args():
    parser = argparse.ArgumentParser(description='Ricoh reporting')
    parser.add_argument('--xlsx',
                        default='printlog.xlsx',
                        help='XLSX file to write')
    parser.add_argument('--db',
                        required=True,
                        help='SQLite3 database from which to read values')
    parser.add_argument('--first',
                        help='First local timestamp "yyyy-mm-dd[ hh:mm:ss]"')
    parser.add_argument('--last',
                        help='Last local timestamp "yyyy-mm-dd[ hh:mm:ss]"')

    parser.add_argument('--logfile',
                        default=None,
                        help='Optional output logfile')

    parser.add_argument('--smtp-recipient',
                        help='Who the email should be sent to')
    parser.add_argument('--smtp-auth-file',
                        help='File containing SMTP AUTH username:password')

    parser.add_argument('--slack-token-filename',
                        help='File containing the Slack bot authorization token')

    parser.add_argument('--google-parent-folder-id',
                        help='If provided, ID of Google Drive folder where to upload the files')
    parser.add_argument('--app-id',
                        default='client_id.json',
                        help='Filename containing Google application ID')
    parser.add_argument('--user-creds',
                        default='user-credentials.json',
                        help='Filename containing Google user credentials')

    parser.add_argument('--verbose',
                        default=False,
                        action='store_true',
                        help='Enable verbose output')
    parser.add_argument('--debug',
                        default=False,
                        action='store_true',
                        help='Enable extra debugging')

    args = parser.parse_args()

    # Sanity check
    if not os.path.exists(args.db):
        print(f"ERROR: SQLite3 database does not exist {args.db}")
        exit(1)

    # It makes no sense to supply some bu not all of the 2 SMTP args
    smtp = 0
    if args.smtp_recipient:
        smtp += 1
    if args.smtp_auth_file:
        smtp += 1
    if not (smtp == 0 or smtp == 2):
        print("ERROR: Must specify either none or all of --smtp-recipient, --smtp-auth-file")
        exit(1)

    # It makes no sense to supply some but not all of the 3 google
    # args
    google = 0
    if args.app_id:
        google += 1
    if args.user_creds:
        google += 1
    if args.google_parent_folder_id:
        google += 1
    if not (google == 0 or google == 3):
        print("ERROR: Must specify either none or all of --app-id, --user-creds, and --google-parent-folder-id")
        exit(1)

    # Convert the string timestamp to a Python datetime in the local
    # timezone
    def _convert_timestamp(timestamp):
        # The timestamp must be of one of the following forms:
        # yyyy-mm-dd
        # yyyy-mm-dd hh:mm:ss
        if re.match('\d\d\d\d-\d\d-\d\d$', timestamp):
            ts = datetime.datetime.strptime(timestamp,
                                            "%Y-%m-%d")

        elif re.match('\d\d\d\d-\d\d-\d\d \d\d:\d\d:\d\d$', timestamp):
            ts = datetime.datetime.strptime(timestamp,
                                            "%Y-%m-%d %H:%M:%S")

        else:
            print(f"ERROR: Timestamp is not in expected format: {timestamp}")
            exit(1)

        # Assume that the time was specified in the local timezone
        local_tz = tzlocal.get_localzone()
        return ts.astimezone(local_tz)

    # Normalize the two timestamps
    if args.first:
        args.first = _convert_timestamp(args.first)
    if args.last:
        args.last  = _convert_timestamp(args.last)

    return args

###########################################################

# Find the first and last timestamps in the database
def find_first_last_timestamp(log, conn):
    def _doit(direction):
        c    = conn.cursor()
        sql  = f'SELECT timestamp FROM printlog GROUP BY timestamp ORDER BY datetime(timestamp) {direction} LIMIT 1'
        log.debug(f"SQL: {sql}")
        c.execute(sql)
        rows = c.fetchall()

        return rows

    log.debug(f"Looking for first and last timestamps")
    rows = _doit("ASC")
    if len(rows) < 1:
        log.verbose("Database is empty: there is no first and last timestamp")
        return None, None

    first = datetime.datetime.fromisoformat(rows[0]['timestamp'])
    log.debug(f"Found first timestamp: {first}")

    rows = _doit("DESC")
    last = datetime.datetime.fromisoformat(rows[0]['timestamp'])
    log.debug(f"Found last timestamp: {last}")

    return first, last

# Find the first timestamp in the database that is >= the requested
# timestamp. Remember that the timestamp parameter is in local time, but
# timestamps in the database are in GMT.
def find_timestamp(log, conn, target_timestamp):
    c = conn.cursor()
    gmt = pytz.timezone("GMT")
    gmt_ts = target_timestamp.astimezone(gmt)

    log.debug(f"Looking for timestamp: {gmt_ts}")
    sql  = f'SELECT timestamp FROM printlog WHERE datetime(timestamp)>=datetime(?) GROUP BY timestamp ORDER BY datetime(timestamp) LIMIT 1'
    log.debug(f"SQL: {sql}")
    c.execute(sql, [gmt_ts])
    rows = c.fetchall()

    log.debug(f"Found {len(rows)} rows greater than {gmt_ts}")
    if len(rows) < 1:
        return None
    return datetime.datetime.fromisoformat(rows[0]['timestamp'])

###########################################################

def fetch_depts_at_timestamp(log, conn, timestamp):
    c     = conn.cursor()
    c.execute('SELECT department FROM printlog WHERE timestamp=?', [timestamp])
    rows  = c.fetchall()
    depts = { row['department'] : timestamp for row in rows }

    log.debug(f"Found {len(depts)} departments at timestamp {timestamp}: {depts}")
    return depts

###########################################################

def compute_depts_union(log, depts_first, depts_last):
    def _compute(depts, label):
        for dept, timestamp in depts.items():
            if dept not in depts_union:
                depts_union[dept] = {
                    'id' : dept,
                }
            depts_union[dept][label] = timestamp

    depts_union = dict()
    _compute(depts_first, "first")
    _compute(depts_last, "last")

    log.debug(f"Union of all departments: {depts_union}")
    return depts_union

###########################################################

# Examine each department:
#
# - if we are missing a "first" timestamp, this means that the department was
#   added between the two timestamps specified on the command line.  Go find
#   the first timestamp where we have data for that new department.
# - if we are missing a "last" timestamp, this means that the department was
#   deleted between the two timestamps specified on the command line.  Go find
#   the last timestamp where we have data for that now-deleted department.
def fetch_missing_timestamps(log, conn, timestamp_first, timestamp_last, depts):
    c = conn.cursor()

    for dept in depts.values():
        label = None
        if 'first' not in dept:
            # Find first timestamp with this department
            desc      = ''
            timestamp = timestamp_first
            label     = 'first'

        elif 'last' not in dept:
            # Find last timestamp with this department
            desc      = 'DESC'
            timestamp = timestamp_last
            label     = 'last'

        if label is not None:
            sql         = f'SELECT timestamp FROM printlog WHERE department=? AND datetime(timestamp)>=datetime(?) GROUP BY timestamp ORDER BY datetime(timestamp) {desc} LIMIT 1'
            values      = [ dept['id'], timestamp ]
            c.execute(sql, values)
            rows        = c.fetchall()
            dept[label] = datetime.datetime.fromisoformat(rows[0]['timestamp'])

    log.debug(f"Depts with all timestamps filled in: {depts}")

###########################################################

# Get the data for each department and timestamp.
#
# We could do something clever to minimize the number of SQL lookups, but
# the Python is simpler to read if we just have a simple loop to look
# up each one.  For a database/application this small, the performance
# difference is negligible.
def fetch_data(log, conn, depts):
    def _fetch(log, id, timestamp):
        sql    = 'SELECT * FROM printlog WHERE department=? AND timestamp=?'
        values = [ id, timestamp ]
        c.execute(sql, values)
        rows = c.fetchall()
        return rows[0]

    c = conn.cursor()

    for dept, item in depts.items():
        log.debug(item)
        data = _fetch(log, dept, item['first'])
        item['first_data'] = data

        data = _fetch(log, dept, item['last'])
        item['last_data'] = data

    log.debug(f"Full set of data: {depts}")

###########################################################

# finds the deltas between the sets of data, returns a list of department deltas
def compute_deltas(log, depts):
    for dept, item in depts.items():
        # Assumption: the column names are the same between the old and new data
        delta = dict()
        for col in item['first_data'].keys():
            data_old = item['first_data'][col]
            data_new = item['last_data'][col]
            # do not compare any data that is not an integer
            if type(data_old) == str and type(data_new) == str:
                continue
            delta[col] = data_new - data_old
        item['deltas'] = delta

    log.debug("Computed deltas")
    log.debug(depts)

###########################################################

# Writes the deltas to an XLSX
def write_to_xlsx(log, fields, depts, filename, timestamp_first, timestamp_last):
    wb = Workbook()
    ws = wb.active

    # Reserve some blank rows for the titles
    titles = [
        'Ricoh printer counts by department',
        # "None" renders the timestamp in the local timezone, and
        # ctime() puts it in a pleasing human-readable format.
        f'{timestamp_first.astimezone(None).ctime()} through {timestamp_last.astimezone(None).ctime()}',
    ]
    for i in range(len(titles) + 1):
        ws.append([])

    # Add a row of the column names
    column_names = [ 'Department', 'Name', 'Start', 'End' ]
    first_dept = list(depts.values())[0]
    item = first_dept['deltas']
    for field in fields:
        if field in item and type(item[field]) != str:
            column_names.append(field)

            # The "bwTotal" and "colorTotal" columns are special:
            # we'll add another column after each of those two, which
            # will be a computed value of this department's percentage
            # of the overall total.
            if field == 'bwTotal' or field == 'colorTotal':
                column_names.append(f'% of overall {field}')

    ws.append(column_names)

    # Get the last row number.  The next row is the first row of data.
    for row in ws.rows:
        # row is a tuple of cells
        cell = row[0]
        last_row = cell.row
    first_data_row = last_row + 1
    last_data_row = first_data_row + len(depts.keys()) - 1

    # Now add a row for each set of delta data
    row = first_data_row
    percentage_cols = list()
    for dept_id in sorted(depts.keys()):
        item  = depts[dept_id]
        # "None" renders these Python datetimes in the local timezone
        first = item['first'].astimezone(None)
        last  = item['last'].astimezone(None)

        # Make sure to render the times the way that we want (e.g., ctime()),
        # otherwise OpenPyXL will render them as GMT.
        data  = [ dept_id, item['first_data']['name'],
                  first.ctime(), last.ctime() ]
        for field in fields:
            if field in item['deltas'] and type(item['deltas'][field]) != str:
                data.append(item['deltas'][field])

                # Just like above, the "bwTotal" and "colorTotal"
                # columns are special: we add in a column for this
                # department's percentage of the overall total.  Note:
                # we don't need to compute this value ourselves -- we
                # just put in an Excel formula to calculate it.
                col_letter = chr(ord('A') + len(data) - 1)
                if field == 'bwTotal' or field == 'colorTotal':
                    value = f'={col_letter}{row}/sum({col_letter}{first_data_row}:{col_letter}{last_data_row})'
                    data.append(value)

                    col_letter = chr(ord('A') + len(data) - 1)
                    if col_letter not in percentage_cols:
                        percentage_cols.append(col_letter)

        ws.append(data)
        row += 1

    # automatically adjusts the width of every column to fit the data in it
    dims = dict()
    for row in ws.rows:
        for cell in row:
            if cell.value:
                dims[cell.column_letter] = max((dims.get(cell.column_letter, 0), len(str(cell.value))))+.5
    for col, value in dims.items():
        ws.column_dimensions[col].width = value

    # Get the last column letter
    for cell in ws[1]:
        last_col = cell.column_letter

    # Get the last row number
    for row in ws.rows:
        # row is a tuple of cells
        cell = row[0]
        last_row = cell.row

    # Set the title rows to a nice value
    for row, title in enumerate(titles):
        ws.merge_cells(f'A{row+1}:{last_col}{row+1}')
        ws[f'A{row+1}'] = title

    # Set the first several rows to be a specific font/color
    title_font = Font(color='FAFAF9')
    title_fill = PatternFill(fgColor='228B22', fill_type='solid')
    title_align = Alignment(horizontal='center')

    # Set the two percentage columns to have "Percent" formats
    for col in percentage_cols:
        for row in range(first_data_row, last_row + 1):
            address = f'{col}{row}'
            ws[address].style = 'Percent'

    # We color all the title rows, the blank row after the titles, and
    # the row with all the column headings
    for row in ws.iter_rows(min_row=1, max_row=len(titles) + 2):
        for cell in row:
            cell.fill = title_fill
            cell.font = title_font

    wb.save(filename)
    log.info(f"Wrote {filename}")

###########################################################

def open_db(log, filename):
    conn = sqlite3.connect(filename)
    conn.row_factory = sqlite3.Row

    # Fetch a list of the SQL table field names.  We do this because we know
    # the SQL table was created with fields in a specific order (per
    # db_insert.py), and we want to output the XLSX with the fields in that
    # order.  Specifically: we use dictionaries (and dictionary-like SQL rows)
    # to store field data here in this script, and those will be iterated in
    # effectively random order.  So we obtain these field names in a list
    # as a stable ordering for emitting the final XLSX.
    c      = conn.cursor()
    c.execute("SELECT * FROM printlog ORDER BY key LIMIT 1")
    fields = [ description[0] for description in c.description ]
    log.debug(f"SQL field names: {fields}")

    return conn, fields

###########################################################

def google_login(args, log):
    log.debug("Logging in to Google...")
    settings = {
        'client_config_backend' : 'file',
        'client_config_file'    : args.app_id,

        'save_credentials'         : True,
        'save_credentials_backend' : 'file',
        'save_credentials_file'    : args.user_creds,

        'get_refresh_token' : True,
    }

    gauth = GoogleAuth(settings=settings)
    gauth.LocalWebserverAuth()

    log.debug("Successfully logged in to Google")

    drive = GoogleDrive(gauth)
    return drive

# Find a folder identified by this name/parent.  If it doesn't
# exist, create it.
def google_find_or_create_folder(drive, folder_name, parent_id, log):
    q = f"'{parent_id}' in parents"
    q += f" and title='{folder_name}'"
    q += f" and mimeType='{Google.mime_types['folder']}'"
    q += " and trashed=false"
    log.debug(f"Query: {q}")
    query = {
        'q': q,
        'corpora': 'allDrives',
    }
    file_list = drive.ListFile(query).GetList()
    for folder in file_list:
        if folder['title'] == folder_name:
            log.debug(f'Found Google Drive target folder: "{folder_name}" (ID: {folder["id"]})')
            return folder

    # If we didn't find it, then go create that folder
    log.debug(f"Google Drive target folder \"{folder_name}\" not found in parent {parent_id} -- need to create it")
    data = {
        'title'    : folder_name,
        'parents'  : [ { 'id': parent_id } ],
        'mimeType' : Google.mime_types['folder'],
    }
    folder = drive.CreateFile(data)
    folder.Upload()
    log.debug(f"Created Google Drive target folder '{folder_name}' (ID: {folder['id']})")
    return folder

def upload_to_google(drive, timestamp, args, log):
    log.info(f'Uploading file "{args.xlsx}" to Google Drive (parent: {args.google_parent_folder_id}')

    # Look for the year folder where we'll upload this file
    year = timestamp.year
    year_folder = google_find_or_create_folder(drive, str(year),
                                               args.google_parent_folder_id,
                                               log)

    # Upload the file into this folder
    basename = os.path.basename(args.xlsx)
    if basename.endswith('.xlsx'):
        basename = basename[:-5]
    metadata = {
        'title'    : basename,
        'mimeType' : Google.mime_types['xlsx'],
        'parents'  : [ {'id': year_folder['id'] } ],
    }

    file = drive.CreateFile(metadata)
    file.SetContentFile(args.xlsx)
    file.Upload({'convert' : True})

    log.info(f'Successfully uploaded Google file: "{basename}" (ID: {file["id"]})')

    return file

###########################################################

def email_results(ts_start, ts_end, args, gfile, log):
    def _ts(ts):
        local_tz = tzlocal.get_localzone()
        local_ts = ts.astimezone(local_tz)
        return local_ts.strftime('%Y-%m-%d %H:%M:%S')

    ts_start = _ts(ts_start)
    ts_end   = _ts(ts_end)

    subject = f'Ricoh data: {ts_start} - {ts_end}'
    url = f'https://docs.google.com/spreadsheets/d/{gfile["id"]}'
    body = f'''<p>The Ricoh Report for {ts_start} - {ts_end} has been generated and uploaded:</p>

<p><a href="{url}">Ricoh data {ts_start} - {ts_end}</a></p>

<p>Your friendly server,<br />
Ronald</p>'''

    ECC.send_email(to_addr=args.smtp_recipient,
                   subject=subject,
                   body=body,
                   log=log,
                   content_type='text/html',
                   from_addr='no-reply@epiphanycatholicchurch.org')

###########################################################

def main():
    args = setup_cli_args()
    log  = ECC.setup_logging(info=args.verbose,
                             debug=args.debug,
                             logfile=args.logfile, rotate=True,
                             slack_token_filename=args.slack_token_filename)
    ECC.setup_email(args.smtp_auth_file, log=log)

    conn, fields = open_db(log, args.db)

    # If the first and last timestamps were not specified, use the first and
    # last timestamps in the database.
    first, last = find_first_last_timestamp(log, conn)
    if args.first == None:
        args.first = first
    if args.last == None:
        args.last = last

    # Find the earliest timestamp in the database that is greater than or equal
    # to the timestamps that were specified on the command line.
    def _check(label, timestamp):
        ts = find_timestamp(log, conn, timestamp)
        if ts is None:
            log.error(f"Could not find suitable timestamp in database for {timestamp}")
            exit(1)
        return ts

    timestamp_first = _check('first', args.first)
    timestamp_last  = _check('last', args.last)

    # Fetch all the departments that are available at those two timestamps.
    depts_first = fetch_depts_at_timestamp(log, conn, timestamp_first)
    depts_last  = fetch_depts_at_timestamp(log, conn, timestamp_last)

    # We may have found a different set of departments at the first and last
    # timestamps (e.g., if a department was added or deleted between the two
    # timestamps).  Compute the union of departments that we found.
    depts = compute_depts_union(log, depts_first, depts_last)

    # Go fetch any missing data (i.e., a department for which we have only a
    # first or last set of data).
    fetch_missing_timestamps(log, conn, timestamp_first, timestamp_last, depts)

    # Now that we have all the timestamps, fetch all the data
    fetch_data(log, conn, depts)

    # Now that we have first and last data for every single department, compute
    # the deltas for all of them.
    compute_deltas(log, depts)

    write_to_xlsx(log, fields, depts, args.xlsx, timestamp_first, timestamp_last)

    # If a Google Drive folder ID was provided, then we need to upload
    # the result
    gfile = None
    if args.google_parent_folder_id:
        drive = google_login(args, log)
        gfile = upload_to_google(drive, timestamp_first, args, log)

    if args.smtp_auth_file:
        email_results(timestamp_first, timestamp_last, args, gfile, log)

    conn.close()

main()
